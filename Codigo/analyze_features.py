import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import pandas as pd

def analyze_features(features_dir):
    """
    Realiza an√°lise explorat√≥ria das caracter√≠sticas extra√≠das
    """
    # Carregar dados
    hog_features = np.load(os.path.join(features_dir, 'hog', 'features.npy'))
    lbp_features = np.load(os.path.join(features_dir, 'lbp', 'features.npy'))
    labels = np.load(os.path.join(features_dir, 'metadata', 'labels.npy'))
    
    print("="*60)
    print("AN√ÅLISE EXPLORAT√ìRIA DAS CARACTER√çSTICAS")
    print("="*60)
    
    # Estat√≠sticas b√°sicas
    print(f"\nüìä Estat√≠sticas B√°sicas:")
    print(f"   Total de amostras: {len(labels)}")
    print(f"   Classes √∫nicas: {len(np.unique(labels))}")
    print(f"   Dimens√µes HOG: {hog_features.shape}")
    print(f"   Dimens√µes LBP: {lbp_features.shape}")
    
    # Distribui√ß√£o de classes
    unique_labels, counts = np.unique(labels, return_counts=True)
    
    # Plotar distribui√ß√£o de classes (top 20)
    plt.figure(figsize=(12, 6))
    
    # Top 20 classes com mais amostras
    sorted_indices = np.argsort(counts)[-20:]
    top_labels = unique_labels[sorted_indices]
    top_counts = counts[sorted_indices]
    
    plt.subplot(1, 2, 1)
    plt.barh(range(len(top_labels)), top_counts)
    plt.yticks(range(len(top_labels)), top_labels)
    plt.xlabel('N√∫mero de Imagens')
    plt.title('Top 20 Pessoas com Mais Imagens')
    plt.grid(axis='x', alpha=0.3)
    
    # Histograma de distribui√ß√£o
    plt.subplot(1, 2, 2)
    plt.hist(counts, bins=30, edgecolor='black', alpha=0.7)
    plt.xlabel('N√∫mero de Imagens por Pessoa')
    plt.ylabel('Frequ√™ncia')
    plt.title('Distribui√ß√£o de Imagens por Pessoa')
    plt.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(features_dir, '..', '..', 'results', 'plots', 'class_distribution.png'), dpi=150)
    plt.show()
    
    # Visualiza√ß√£o t-SNE para HOG features
    print("\nüîç Redu√ß√£o de dimensionalidade (t-SNE)...")
    
    # Amostrar dados para t-SNE (muito intensivo computacionalmente)
    n_samples = min(1000, len(labels))
    indices = np.random.choice(len(labels), n_samples, replace=False)
    
    # Aplicar PCA primeiro para reduzir dimensionalidade
    pca = PCA(n_components=50)
    hog_pca = pca.fit_transform(hog_features[indices])
    
    # Aplicar t-SNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    hog_tsne = tsne.fit_transform(hog_pca)
    
    # Plotar t-SNE
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(hog_tsne[:, 0], hog_tsne[:, 1], 
                         c=pd.factorize(labels[indices])[0], 
                         cmap='tab20', alpha=0.6, s=10)
    plt.colorbar(scatter)
    plt.title('Visualiza√ß√£o t-SNE das Caracter√≠sticas HOG')
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    plt.savefig(os.path.join(features_dir, '..', '..', 'results', 'plots', 'tsne_hog.png'), dpi=150)
    plt.show()
    
    # Matriz de correla√ß√£o das caracter√≠sticas HOG (primeiras 20)
    print("\nüìà Matriz de correla√ß√£o das caracter√≠sticas HOG...")
    
    plt.figure(figsize=(10, 8))
    corr_matrix = np.corrcoef(hog_features[:100, :20].T)  # Primeiras 20 features de 100 amostras
    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, 
                square=True, linewidths=.5, cbar_kws={"shrink": .8})
    plt.title('Matriz de Correla√ß√£o (Primeiras 20 Features HOG)')
    plt.tight_layout()
    plt.savefig(os.path.join(features_dir, '..', '..', 'results', 'plots', 'correlation_matrix.png'), dpi=150)
    plt.show()
    
    # Histogramas de valores de caracter√≠sticas
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Histograma HOG
    axes[0, 0].hist(hog_features.flatten(), bins=50, alpha=0.7, edgecolor='black')
    axes[0, 0].set_title('Distribui√ß√£o dos Valores HOG')
    axes[0, 0].set_xlabel('Valor')
    axes[0, 0].set_ylabel('Frequ√™ncia')
    axes[0, 0].grid(alpha=0.3)
    
    # Histograma LBP
    axes[0, 1].hist(lbp_features.flatten(), bins=50, alpha=0.7, edgecolor='black', color='orange')
    axes[0, 1].set_title('Distribui√ß√£o dos Valores LBP')
    axes[0, 1].set_xlabel('Valor')
    axes[0, 1].set_ylabel('Frequ√™ncia')
    axes[0, 1].grid(alpha=0.3)
    
    # Boxplot HOG por classe (primeiras 5 classes)
    axes[1, 0].boxplot([hog_features[labels == label][:100, 0] for label in unique_labels[:5]])
    axes[1, 0].set_xticklabels(unique_labels[:5], rotation=45)
    axes[1, 0].set_title('Boxplot da Primeira Feature HOG por Classe')
    axes[1, 0].set_ylabel('Valor da Feature')
    axes[1, 0].grid(alpha=0.3)
    
    # Vari√¢ncia explicada por PCA
    pca_full = PCA().fit(hog_features)
    explained_variance = np.cumsum(pca_full.explained_variance_ratio_)
    
    axes[1, 1].plot(explained_variance, marker='o')
    axes[1, 1].set_xlabel('N√∫mero de Componentes PCA')
    axes[1, 1].set_ylabel('Vari√¢ncia Acumulada Explicada')
    axes[1, 1].set_title('Vari√¢ncia Explicada por Componentes PCA')
    axes[1, 1].axhline(y=0.95, color='r', linestyle='--', alpha=0.5)
    axes[1, 1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(features_dir, '..', '..', 'results', 'plots', 'feature_analysis.png'), dpi=150)
    plt.show()
    
    # Relat√≥rio final
    print("\n" + "="*60)
    print("RELAT√ìRIO DA AN√ÅLISE")
    print("="*60)
    
    # Calcular informa√ß√µes importantes
    n_components_95 = np.argmax(explained_variance >= 0.95) + 1
    print(f"\nüìã Informa√ß√µes para modelagem:")
    print(f"   Componentes PCA para 95% vari√¢ncia: {n_components_95}")
    print(f"   Dimens√£o original HOG: {hog_features.shape[1]}")
    print(f"   Redu√ß√£o poss√≠vel: {hog_features.shape[1] / n_components_95:.1f}x")
    
    # Verificar balanceamento
    imbalance_ratio = counts.max() / counts.min()
    print(f"\n‚öñÔ∏è Balanceamento das classes:")
    print(f"   Raz√£o de desbalanceamento: {imbalance_ratio:.2f}")
    if imbalance_ratio > 10:
        print("   ‚ö†Ô∏è  Aten√ß√£o: Classes muito desbalanceadas!")
    elif imbalance_ratio > 5:
        print("   ‚ö†Ô∏è  Classes moderadamente desbalanceadas")
    else:
        print("   ‚úì Classes razoavelmente balanceadas")
    
    # Salvar relat√≥rio
    report_path = os.path.join(features_dir, '..', '..', 'results', 'logs', 'feature_analysis_report.txt')
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w') as f:
        f.write("RELAT√ìRIO DE AN√ÅLISE DE CARACTER√çSTICAS\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"Total de amostras: {len(labels)}\n")
        f.write(f"Classes √∫nicas: {len(unique_labels)}\n")
        f.write(f"Dimens√µes HOG: {hog_features.shape}\n")
        f.write(f"Dimens√µes LBP: {lbp_features.shape}\n")
        f.write(f"\nDistribui√ß√£o de classes:\n")
        for label, count in zip(unique_labels[:10], counts[:10]):
            f.write(f"  {label}: {count} imagens\n")
        f.write(f"\nComponentes PCA para 95% vari√¢ncia: {n_components_95}\n")
        f.write(f"Raz√£o de desbalanceamento: {imbalance_ratio:.2f}\n")

if __name__ == "__main__":
    # Definir caminho
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    features_dir = os.path.join(base_dir, "data", "features")
    
    if not os.path.exists(features_dir):
        print(f"Erro: Diret√≥rio de caracter√≠sticas n√£o encontrado: {features_dir}")
        print("Execute primeiro extract_features.py")
    else:
        analyze_features(features_dir)